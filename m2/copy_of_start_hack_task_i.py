# -*- coding: utf-8 -*-
"""Copy of START HACK  - Task I

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AxfEUoT3-mbgYnp9o-KccEdq1xcWxLoh

# Uploading the Kaggle API Token
In this cell, you will upload your Kaggle API token to authenticate and access the Kaggle dataset.
The `kaggle.json` file, which contains your API credentials, should be downloaded from your Kaggle account.
To download it, go to your Kaggle account settings, scroll to the API section, and click 'Create New API Token'.
Once you have your `kaggle.json` file, click the 'Browse' button below to upload it to this notebook environment.
"""

# from google.colab import files
# files.upload()

"""# Setting Up Kaggle API Credentials
After uploading your `kaggle.json` file, this cell will run commands to set up the environment
variables so that the Kaggle API can use these credentials. This includes making a `.kaggle`
directory at the root, copying the `kaggle.json` file into this directory, and setting the
appropriate permissions for the file.
"""

# !mkdir -p ~/.kaggle
# !cp kaggle.json ~/.kaggle/
# !chmod 600 ~/.kaggle/kaggle.json

"""# Downloading the Wildfire Dataset
This cell will handle the downloading of the LAST version of the wildfire dataset from Kaggle.
Once executed, the dataset will be downloaded
and saved to the current working directory in Colab.
"""

# !kaggle datasets download -d

"""# Unzipping the Dataset
After downloading the dataset, this cell will unzip the dataset. The dataset comes in a
compressed `.zip` file format to save space. Running the command below will extract the
files into a directory that you can then work with in the notebook.
"""

# !unzip the-wildfire-dataset.zip

"""## Setting Up Data Loaders

Before we can start training our model, we need to load our image data into a format that's usable by TensorFlow. This is done by using the `tf.keras.utils.image_dataset_from_directory` function, which conveniently handles images stored in a directory structure.

Here's what we are setting up in this cell:

1. **Directory Paths**: We define the base directory where our dataset is located. Within this base directory, we should have three subdirectories corresponding to our training, validation, and test sets, respectively.

2. **Image Loading Parameters**: We set the batch size and image size. The batch size is the number of images that the model will process at one time during training, and the image size is the dimensions to which all images will be resized.

3. **Data Loaders**: We create data loaders for the training, validation, and test datasets. These loaders will read the images from disk, apply the specified preprocessing, and arrange them into batches.

Note: The `label_mode` is set to 'binary' which indicates that our problem is a binary classification task. If you have more than two classes, change this parameter to 'categorical'.

The `shuffle` parameter is set to `True` for the training dataset to ensure that the model does not learn anything from the order of the images. It's not necessary to shuffle the validation and test sets, but doing so does not harm and ensures that any evaluation of the model is done on a random sample of the data.

After running this cell, `train_dataset`, `val_dataset`, and `test_dataset` will be ready for use in the model training and evaluation steps.
"""

import os
import tensorflow as tf
from tensorflow.keras import layers, models

# Define directory paths and constants
IMG_SIZE = (224, 224)
BATCH_SIZE = 32
learning_rate_initial = 0.01

# Define the directory paths for the training, validation, and test sets
base_dir = '../the_wildfire_dataset_2n_version'
train_dir = os.path.join(base_dir, 'train')
val_dir = os.path.join(base_dir, 'val')
test_dir = os.path.join(base_dir, 'test')

# Load the datasets using image_dataset_from_directory
train_dataset = tf.keras.utils.image_dataset_from_directory(
    train_dir,
    shuffle=True,
    batch_size=BATCH_SIZE,
    image_size=IMG_SIZE,
    label_mode='binary'  # Use 'binary' because there are only two classes
)

val_dataset = tf.keras.utils.image_dataset_from_directory(
    val_dir,
    shuffle=True,
    batch_size=BATCH_SIZE,
    image_size=IMG_SIZE,
    label_mode='binary'
)

test_dataset = tf.keras.utils.image_dataset_from_directory(
    test_dir,
    shuffle=True,
    batch_size=BATCH_SIZE,
    image_size=IMG_SIZE,
    label_mode='binary'
)

class_names = train_dataset.class_names
label_map = {class_names.index('fire'): 1, class_names.index('nofire'): 0}

# Define a function to remap the labels
def remap_labels(image, label):
    return image, tf.where(label == 0, 1, 0)  # this will swap 0s and 1s

# Use the `map` function to apply the remap_labels function
train_dataset = train_dataset.map(remap_labels)
val_dataset = val_dataset.map(remap_labels)
test_dataset = test_dataset.map(remap_labels)

import matplotlib.pyplot as plt

def show_batch(image_batch, label_batch):
    plt.figure(figsize=(10,10))
    for n in range(min(len(image_batch), 9)):  # Display up to 9 images from the batch
        ax = plt.subplot(3, 3, n+1)
        plt.imshow(image_batch[n] / 255.0)  # Make sure the pixel values are in [0,1] range
        if label_batch[n]:
            plt.title("Fire")
        else:
            plt.title("No Fire")
        plt.axis("off")

image_batch, label_batch = next(iter(train_dataset))
show_batch(image_batch, label_batch)

"""## Optimizing Data Loading

In this cell, we optimize the data loading process for our training, validation, and test datasets. `AUTOTUNE` is a TensorFlow feature that allows the dataset to dynamically adjust the number of images it processes simultaneously, based on the current system conditions.

By calling the `prefetch` method with `buffer_size=AUTOTUNE`, we enable the input pipeline to fetch batches in the background while the model is training. This helps in reducing the time the model has to wait for the data and can improve overall training speed.

Prefetching is a best practice that can significantly improve computational efficiency.
"""

# Configure the datasets for performance
train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)
val_dataset = val_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)

"""## Setting Up the Base Model with EfficientNetB0

To leverage the power of transfer learning, we're using a pre-trained model, EfficientNetB0, as the starting point for our own image classification task. Pre-trained on ImageNet, EfficientNetB0 has learned rich feature representations for a wide range of images which can be beneficial for our specific task.
After pre-training and feature extraction, we might want to fine-tune the base model to better suit our specific task. Fine-tuning can lead to significant improvements in accuracy by slightly adjusting the weights of the pre-trained model.

"""

# Define the model
base_model = tf.keras.applications.EfficientNetB0(
    input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3),
    include_top=False,
    weights='imagenet'
)
base_model.trainable = True  # Fine-tune the base model

"""## Fine-Tuning the Model

Fine-tuning is a powerful technique to optimize the pre-trained model for our specific task, allowing us to adjust the higher-order feature representations in the pre-trained model to make them more relevant for the classification of our images.
"""

# Freeze all the layers before the `fine_tune_at` layer
fine_tune_at = 1
for layer in base_model.layers[:fine_tune_at]:
    layer.trainable = False

"""## Incorporating a Global Average Pooling Layer

After extracting features using the base model, we often have a high-dimensional feature map for each image. To simplify the model and reduce its complexity, we can apply a Global Average Pooling (GAP) layer. The GAP layer reduces each feature map to a single value by calculating the average of all values in the feature map. This process significantly reduces the dimensionality and prepares the data for the final classification layer. To consolidate the features extracted by the base model into a more manageable form, we introduce a Global Average Pooling 2D (GAP) layer. This layer reduces the dimensions of the feature maps from the base model by calculating the average value for each feature map channel. This step is crucial for reducing the model's complexity and computational load without sacrificing the essential information needed for accurate predictions.

"""

inputs = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))
x = base_model(inputs, training=True)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dropout(0.2)(x)
outputs = layers.Dense(1, activation='sigmoid')(x)  # Use 'sigmoid' for binary classification

"""## Building the Complete Model

With all the components ready, we now assemble the complete model. This model incorporates preprocessing, the pre-trained base model for feature extraction, pooling to condense the features, and a prediction layer for output. Additionally, we include a Dropout layer to reduce overfitting by randomly setting input units to 0 at a rate of 0.2 during training.
"""

# Build the model
model = models.Model(inputs, outputs)

"""## Compiling the Model

With our model architecture defined, the next step is to compile the model. Compiling the model prepares it for training by specifying the loss function, optimizer, and metrics to use for evaluation.

"""

# Compile the model
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate_initial/10),
    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),
    metrics=['accuracy']
)

"""## Training the Model

Now that our model is compiled, we're ready to start the training process. Training the model involves feeding it our training dataset and letting it learn to predict the correct labels. During training, the model will also be validated using a separate dataset to monitor its performance on data it has not seen before.


"""

# Fit the model
history = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=15
)

"""## Saving Your Trained Model to Google Drive

After training your model, it's important to save it so that you can share it with the hackathon organizers and use it for further evaluation. In the following cell, we mount your Google Drive for easy file access and save your trained model directly to it. This ensures that your model is stored securely and can be accessed from anywhere, anytime.
"""

# from google.colab import drive
# drive.mount('/content/drive')

# Replace 'my_model' with your model's name and choose an appropriate path in your Google Drive
model_name = 'my_model.h5'
save_path = f'./models/{model_name}'

# Save your model
model.save(save_path)
print(f'Model saved to {save_path}')

"""## Evaluating the Trained Model on the test dataset

With your model now trained and saved, the next critical step is to assess its performance on a new set of data. This evaluation will provide you with valuable insights into how well your model generalizes to unseen examples, reflecting its potential effectiveness in real-world scenarios. In the following code cell, we will use the .evaluate() method, which will output the model's accuracy and loss on the dataset. It's important to note that this step is crucial for understanding your model's strengths and areas for improvement.
"""

# Evaluate the model
model.evaluate(test_dataset)

"""## Important: Protect Your API Key

Before submitting this notebook, please ensure that you remove your `kaggle.json` file from the notebook environment. To do this, you can simply delete the cell that uploaded the `kaggle.json` file and any cells that may have outputted its contents.

Furthermore, before pushing this notebook to GitHub or any other public repository, ensure that your API key is not visible or stored in the code. Do not share or expose your credentials in any way within this notebook.

**To remove the kaggle.json file from your Colab environment, you can use the following command:**

```python
!rm -rf ~/.kaggle/kaggle.json

"""